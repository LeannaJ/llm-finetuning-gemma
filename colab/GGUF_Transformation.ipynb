{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 0. í™˜ê²½ ì„¤ì • (í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜)\n",
        "# ====================================================\n",
        "!pip install -U accelerate peft bitsandbytes\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "5_PgoaufbEib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers repo clone ë° ì„¤ì¹˜\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "ImYk8ux1kqgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi, upload_file\n",
        "login()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P4upvd8ChUDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 1. ì‚¬ìš©ì ì„¤ì •\n",
        "# ====================================================\n",
        "model_list = [\n",
        "    {\n",
        "        \"model_id\": \"LeannaJ/gemma3n-lora-summary\",\n",
        "        \"repo_id_gguf\": \"LeannaJ/gemma3n-lora-gguf-summary\",\n",
        "        \"local_model_path\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_input_summary\",\n",
        "        \"gguf_output_dir\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_output_summary\"\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"LeannaJ/gemma3n-lora-math\",\n",
        "        \"repo_id_gguf\": \"LeannaJ/gemma3n-lora-gguf-math\",\n",
        "        \"local_model_path\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_input_math\",\n",
        "        \"gguf_output_dir\": \"/content/drive/MyDrive/Gemma_FineTuning/GGUF/gguf_output_math\"\n",
        "    }\n",
        "]\n",
        "\n",
        "quant_method = \"q4_0\"  # ë˜ëŠ” \"f16\", \"q8_0\""
      ],
      "metadata": {
        "id": "F4IWIgLTbH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 2. GGUF ë³€í™˜ ì‹¤í–‰ (transformers/scripts/convert/gguf/convert.py)\n",
        "# ====================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os, glob\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "for m in model_list:\n",
        "    print(f\"\\nğŸš€ ë³€í™˜ ì‹œì‘: {m['model_id']}\")\n",
        "\n",
        "    # ë””ë ‰í† ë¦¬ ì¤€ë¹„\n",
        "    os.makedirs(m[\"local_model_path\"], exist_ok=True)\n",
        "    os.makedirs(m[\"gguf_output_dir\"], exist_ok=True)\n",
        "\n",
        "    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥\n",
        "    tokenizer = AutoTokenizer.from_pretrained(m[\"model_id\"])\n",
        "    model = AutoModelForCausalLM.from_pretrained(m[\"model_id\"])\n",
        "    tokenizer.save_pretrained(m[\"local_model_path\"])\n",
        "    model.save_pretrained(m[\"local_model_path\"])\n",
        "\n",
        "    # GGUF ë³€í™˜ ì‹¤í–‰\n",
        "    !python3 scripts/convert/gguf/convert.py \\\n",
        "      --model_path {m[\"local_model_path\"]} \\\n",
        "      --outfile_path {m[\"gguf_output_dir\"]} \\\n",
        "      --quantize {quant_method}\n",
        "\n",
        "    print(f\"âœ… GGUF ë³€í™˜ ì™„ë£Œ: {m['gguf_output_dir']}\")\n",
        "\n",
        "    # GGUF íŒŒì¼ ì—…ë¡œë“œ\n",
        "    gguf_files = glob.glob(f\"{m['gguf_output_dir']}/*.gguf\")\n",
        "    api.create_repo(m[\"repo_id_gguf\"], exist_ok=True)\n",
        "\n",
        "    for filepath in gguf_files:\n",
        "        filename = os.path.basename(filepath)\n",
        "        upload_file(\n",
        "            path_or_fileobj=filepath,\n",
        "            path_in_repo=filename,\n",
        "            repo_id=m[\"repo_id_gguf\"],\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename} â†’ {m['repo_id_gguf']}\")\n",
        "\n",
        "    print(f\"ğŸŒ ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{m['repo_id_gguf']}\")"
      ],
      "metadata": {
        "id": "6MY0pLWJDPRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}