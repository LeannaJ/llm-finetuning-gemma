# Gemma 3n 2B IT - CNN/DailyMail μ”μ•½ ν‰κ°€ νμΈνλ‹

## π“ ν”„λ΅μ νΈ κ°μ”

μ΄ ν”„λ΅μ νΈλ” Gemma 3n 2B IT λ¨λΈμ„ CNN/DailyMail λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•μ—¬ κµμ΅μ© μ”μ•½ ν‰κ°€ μ‹μ¤ν…μ„ κµ¬μ¶•ν•λ” κ²ƒμ…λ‹λ‹¤.

### π― λ©ν‘
- **1λ‹¨κ³„**: μ”μ•½ν‰κ°€ - κΈ°μ‚¬ β†’ ν•™μƒ μ”μ•½ β†’ LLM ν‰κ°€ λ¨λΈ ν•™μµ
- **2λ‹¨κ³„**: μ£Όμ λ¶„λ¥ - κΈ°μ‚¬ β†’ ν•™μƒμ΄ μ£Όμ λ¥Ό κ³ λ¥΄κ±°λ‚ μ„¤λ… β†’ μ •ν™•λ„ μΈ΅μ •
- **3λ‹¨κ³„**: λΉ„νμ  κΈ€μ“°κΈ° - κΈ°μ‚¬ + μ§λ¬Έ β†’ μ—μ„Έμ΄ μƒμ„± λ° μ±„μ 
- **4λ‹¨κ³„**: Pairwise Tuning - μ¤λ¦¬μ , λ―Όκ°ν• μ΄μμ— λ€ν• κµμ΅μ  κ°€μΉ νμΈνλ‹

## π› οΈ κΈ°μ  μ¤νƒ

- **λ¨λΈ**: Google Gemma 3n 2B IT
- **νμΈνλ‹**: LoRA (Low-Rank Adaptation)
- **λ°μ΄ν„°μ…‹**: CNN/DailyMail 3.0.0
- **ν”„λ μ„μ›ν¬**: PyTorch, Transformers, PEFT, TRL
- **ν‰κ°€**: ROUGE Score, Custom Evaluation Metrics

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
finetune/
β”β”€β”€ finetune_gemma_cnn_dailymail.ipynb    # μ½”λ© λ…ΈνΈλ¶ (μ™„μ „ν• νμ΄ν”„λΌμΈ)
β”β”€β”€ finetune_config.py                    # μ„¤μ • κ΄€λ¦¬
β”β”€β”€ finetune_utils.py                     # μ ν‹Έλ¦¬ν‹° ν•¨μλ“¤
β”β”€β”€ run_finetune.py                       # μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”β”€β”€ finetune_requirements.txt             # ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬
β””β”€β”€ FINETUNE_README.md                    # μ΄ νμΌ
```

## π€ λΉ λ¥Έ μ‹μ‘

### 1. μ½”λ©μ—μ„ μ‹¤ν–‰ (κ¶μ¥)

1. **λ…ΈνΈλ¶ μ—…λ΅λ“**: `finetune_gemma_cnn_dailymail.ipynb`λ¥Ό μ½”λ©μ— μ—…λ΅λ“
2. **GPU λ°νƒ€μ„ μ„¤μ •**: Runtime β†’ Change runtime type β†’ GPU
3. **λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ**: μ²« λ²μ§Έ μ…€ μ‹¤ν–‰
4. **νμΈνλ‹ μ‹¤ν–‰**: μ „μ²΄ λ…ΈνΈλ¶ μμ°¨ μ‹¤ν–‰

### 2. λ΅μ»¬μ—μ„ μ‹¤ν–‰

```bash
# 1. κ°€μƒν™κ²½ μƒμ„±
python -m venv finetune_env
source finetune_env/bin/activate  # Windows: finetune_env\Scripts\activate

# 2. λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ
pip install -r finetune_requirements.txt

# 3. νμΈνλ‹ μ‹¤ν–‰
python run_finetune.py
```

## β™οΈ μ„¤μ •

### κΈ°λ³Έ μ„¤μ • (`finetune_config.py`)

```python
# λ¨λΈ μ„¤μ •
model_name = "google/gemma-3n-2b-it"
max_seq_length = 2048
load_in_8bit = True

# LoRA μ„¤μ •
r = 16
lora_alpha = 32
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# ν›λ ¨ μ„¤μ •
num_train_epochs = 3
per_device_train_batch_size = 2
learning_rate = 2e-4

# λ°μ΄ν„° μ„¤μ •
max_train_samples = 2000
max_val_samples = 500
max_article_length = 2000
```

### μ„¤μ • μ»¤μ¤ν„°λ§μ΄μ§•

```python
from finetune_config import FinetuneConfig

# μ„¤μ • μμ •
config = FinetuneConfig()
config.training.num_train_epochs = 5
config.training.learning_rate = 1e-4
config.data.max_train_samples = 5000

# νμΈνλ‹ μ‹¤ν–‰
from run_finetune import main
main()
```

## π“ λ°μ΄ν„° κµ¬μ΅°

### μ…λ ¥ ν•μ‹
```
<|system|>
λ‹Ήμ‹ μ€ κµμ΅ μ „λ¬Έκ°€μ…λ‹λ‹¤. ν•™μƒμ μ”μ•½μ„ ν‰κ°€ν•κ³  μ μμ™€ ν”Όλ“λ°±μ„ μ κ³µν•΄μ£Όμ„Έμ”.

ν‰κ°€ κΈ°μ¤€:
1. ν•µμ‹¬ λ‚΄μ© ν¬ν•¨λ„ (40μ ): μ£Όμ” μ‚¬μ‹¤κ³Ό μ •λ³΄κ°€ ν¬ν•¨λμ—λ”κ°€?
2. μ •ν™•μ„± (30μ ): μ •λ³΄κ°€ μ •ν™•ν•κ² μ „λ‹¬λμ—λ”κ°€?
3. κ°„κ²°μ„± (20μ ): λ¶ν•„μ”ν• λ‚΄μ© μ—†μ΄ κ°„κ²°ν•κ°€?
4. λ¬Έλ²• λ° ν‘ν„ (10μ ): λ¬Έλ²•μ μΌλ΅ μ¬λ°”λ¥΄κ³  μμ—°μ¤λ¬μ΄κ°€?

μ΄μ : 100μ  λ§μ 
<|user|>
λ‹¤μ κΈ°μ‚¬λ¥Ό μ½κ³  3-4λ¬Έμ¥μΌλ΅ μ”μ•½ν•΄μ£Όμ„Έμ”:

[κΈ°μ‚¬ λ‚΄μ©]

<|assistant|>
κΈ°μ‚¬λ¥Ό μ”μ•½ν•΄λ“λ¦¬κ² μµλ‹λ‹¤:

[ν•™μƒ μ”μ•½]

<|user|>
μ„ μ”μ•½μ„ ν‰κ°€ν•΄μ£Όμ„Έμ”. μ •λ‹µ μ”μ•½μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:

[μ •λ‹µ μ”μ•½]

<|assistant|>
μ”μ•½ ν‰κ°€ κ²°κ³Ό:

**μ΄μ : [μ μ]/100**

**μ„Έλ¶€ ν‰κ°€:**
- ν•µμ‹¬ λ‚΄μ© ν¬ν•¨λ„: [μ μ]/40 - [ν”Όλ“λ°±]
- μ •ν™•μ„±: [μ μ]/30 - [ν”Όλ“λ°±]  
- κ°„κ²°μ„±: [μ μ]/20 - [ν”Όλ“λ°±]
- λ¬Έλ²• λ° ν‘ν„: [μ μ]/10 - [ν”Όλ“λ°±]

**μ „μ²΄ ν”Όλ“λ°±:**
[μ „μ²΄μ μΈ ν”Όλ“λ°±κ³Ό κ°μ„ μ ]

**κ°μ„  μ μ•:**
[κµ¬μ²΄μ μΈ κ°μ„  λ°©ν–¥]
```

## π”§ μ£Όμ” κΈ°λ¥

### 1. λ°μ΄ν„° μ „μ²λ¦¬
- CNN/DailyMail λ°μ΄ν„°μ…‹ μλ™ λ΅λ“
- κΈ°μ‚¬ κΈΈμ΄ ν•„ν„°λ§ λ° μ ν•
- κµμ΅μ© ν”„λ΅¬ν”„νΈ μλ™ μƒμ„±

### 2. LoRA νμΈνλ‹
- ν¨μ¨μ μΈ νλΌλ―Έν„° ν•™μµ (μ•½ 1%λ§ ν›λ ¨)
- 8bit μ–‘μν™”λ΅ λ©”λ¨λ¦¬ ν¨μ¨μ„± ν–¥μƒ
- Early stoppingμΌλ΅ κ³Όμ ν•© λ°©μ§€

### 3. ν‰κ°€ μ‹μ¤ν…
- ROUGE μ μ μλ™ κ³„μ‚°
- μ»¤μ¤ν…€ ν‰κ°€ λ©”νΈλ¦­
- μ‹¤μ‹κ°„ λ¨λΈ μ„±λ¥ λ¨λ‹ν„°λ§

### 4. λ¨λΈ μ €μ¥ λ° λ΅λ“
- ν›λ ¨λ λ¨λΈ μλ™ μ €μ¥
- μ„¤μ • νμΌ JSON ν•νƒλ΅ μ €μ¥
- HuggingFace Hub μ—…λ΅λ“ μ§€μ›

## π“ μ„±λ¥ μµμ ν™”

### λ©”λ¨λ¦¬ μµμ ν™”
- 8bit μ–‘μν™” μ‚¬μ©
- LoRAλ΅ νλΌλ―Έν„° μ κ°μ†
- Gradient accumulationμΌλ΅ λ°°μΉ ν¬κΈ° μ΅°μ 

### ν›λ ¨ μµμ ν™”
- Mixed precision training (FP16)
- Early stopping
- Learning rate scheduling

### ν‰κ°€ μµμ ν™”
- λ°°μΉ λ‹¨μ„ ν‰κ°€
- ROUGE μ μ μΊμ‹±
- λ³‘λ ¬ μ²λ¦¬

## π§ μ‹¤ν— λ° ν‰κ°€

### ν‰κ°€ λ©”νΈλ¦­
1. **ROUGE Score**: μ”μ•½ ν’μ§ ν‰κ°€
   - ROUGE-1: λ‹¨μ–΄ λ‹¨μ„ μ¤‘λ³µ
   - ROUGE-2: 2-gram μ¤‘λ³µ
   - ROUGE-L: μµμ¥ κ³µν†µ λ¶€λ¶„μμ—΄

2. **μ»¤μ¤ν…€ ν‰κ°€**: κµμ΅μ  κ΄€μ 
   - ν•µμ‹¬ λ‚΄μ© ν¬ν•¨λ„ (40μ )
   - μ •ν™•μ„± (30μ )
   - κ°„κ²°μ„± (20μ )
   - λ¬Έλ²• λ° ν‘ν„ (10μ )

### μ‹¤ν— μ„¤μ •
```python
# λ‹¤μ–‘ν• μ‹¤ν— μ„¤μ •
experiments = {
    "baseline": {"lr": 2e-4, "epochs": 3, "samples": 2000},
    "high_lr": {"lr": 5e-4, "epochs": 3, "samples": 2000},
    "more_data": {"lr": 2e-4, "epochs": 3, "samples": 5000},
    "more_epochs": {"lr": 2e-4, "epochs": 5, "samples": 2000}
}
```

## π”„ λ‹¤μ λ‹¨κ³„

### 1. 2λ‹¨κ³„ - μ£Όμ λ¶„λ¥ (AG News)
```python
# AG News λ°μ΄ν„°μ…‹ ν™μ©
dataset_name = "ag_news"
# κΈ°μ‚¬ β†’ μ£Όμ  λ¶„λ¥ β†’ μ •ν™•λ„ ν‰κ°€
```

### 2. 3λ‹¨κ³„ - λΉ„νμ  κΈ€μ“°κΈ°
```python
# μ§λ¬Έ μƒμ„± λ° μ—μ„Έμ΄ ν‰κ°€
# κΈ°μ‚¬ + "μ΄ κΈ°μ‚¬μ— λ™μν•λ‚μ”?" β†’ μ—μ„Έμ΄ β†’ ν‰κ°€
```

### 3. 4λ‹¨κ³„ - Pairwise Tuning
```python
# κµμ΅μ  κ°€μΉμ™€ μ¤λ¦¬μ  κ³ λ ¤μ‚¬ν•­
# λ―Όκ°ν• μ£Όμ μ— λ€ν• μ μ ν• λ‹µλ³€ μƒμ„±
```

## π¨ μ£Όμμ‚¬ν•­

### ν•λ“μ›¨μ–΄ μ”κµ¬μ‚¬ν•­
- **μµμ†**: 8GB GPU λ©”λ¨λ¦¬
- **κ¶μ¥**: 16GB+ GPU λ©”λ¨λ¦¬ (T4, V100, A100)
- **CPU**: μµμ† 4μ½”μ–΄, κ¶μ¥ 8μ½”μ–΄+

### λ©”λ¨λ¦¬ μ‚¬μ©λ‰
- λ¨λΈ λ΅λ“: ~4GB
- ν›λ ¨ μ¤‘: ~6-8GB
- λ°°μΉ ν¬κΈ° μ΅°μ λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ΅°μ • κ°€λ¥

### μ‹κ°„ μμƒ
- 2000 μƒν”, 3 μ—ν¬ν¬: ~2-3μ‹κ°„ (T4 κΈ°μ¤€)
- 5000 μƒν”, 5 μ—ν¬ν¬: ~6-8μ‹κ°„ (T4 κΈ°μ¤€)

## π“ λ¬Έμ  ν•΄κ²°

### μΌλ°μ μΈ λ¬Έμ λ“¤

1. **CUDA out of memory**
   ```python
   # λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
   config.training.per_device_train_batch_size = 1
   config.training.gradient_accumulation_steps = 8
   ```

2. **ν† ν¬λ‚μ΄μ € μ¤λ¥**
   ```python
   # ν¨λ”© ν† ν° μ„¤μ • ν™•μΈ
   if tokenizer.pad_token is None:
       tokenizer.pad_token = tokenizer.eos_token
   ```

3. **λ°μ΄ν„°μ…‹ λ΅λ“ μ‹¤ν¨**
   ```python
   # μΈν„°λ„· μ—°κ²° ν™•μΈ
   # HuggingFace ν† ν° μ„¤μ • (ν•„μ”μ‹)
   ```

## π“ μ°Έκ³  μλ£

- [Gemma λ¨λΈ λ¬Έμ„](https://huggingface.co/google/gemma-3n-2b-it)
- [LoRA λ…Όλ¬Έ](https://arxiv.org/abs/2106.09685)
- [CNN/DailyMail λ°μ΄ν„°μ…‹](https://huggingface.co/datasets/cnn_dailymail)
- [PEFT λ¬Έμ„](https://huggingface.co/docs/peft)
- [TRL λ¬Έμ„](https://huggingface.co/docs/trl)

## π¤ κΈ°μ—¬ν•κΈ°

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## π“„ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤.

---

**Happy Fine-tuning! π€** 